---
title: 'MultiLangMemeNet: A Unified Multimodal Approach for Cross-Lingual Meme Sentiment Analysis'

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here
# and it will be replaced with their full name and linked to their profile.
authors:
  - admin
  - Shakib Sadat Shanto
  - Zishan Ahmed
  - Ahmed Shakib Reza

# Author notes (optional)
author_notes:
  - 'Equal contribution'
  - 'Equal contribution'

date: '2024-11-01T00:00:00Z'
doi: ''

# Schedule page publish date (NOT publication's date).
publishDate: '2017-01-01T00:00:00Z'

# Publication type.
# Accepts a single type but formatted as a YAML list (for Hugo requirements).
# Enter a publication type from the CSL standard.
publication_types: ['paper-conference']

# Publication name and optional abbreviated publication name.
publication: In *23rd International Conference on Machine Learning and Applications*
publication_short: In *ICMLA'24*

abstract: This study introduces MultiLangMemeNet, a novel, unified multimodal approach for meme sentiment classification across diverse languages. The proposed model integrates visual and textual components to effectively capture the multimodal nature of memes. Five language datasets—English, Bengali, Chinese, Hindi, and Tamil—were used for the experiments. In every language examined, MultiLangMemeNet performed consistently better than both state-of-the-art multimodal techniques and unimodal baselines. The accuracy gains ranged from 2.46% to 13.74%, indicating significant improvements over the top unimodal vision and text models achieved by the model. Furthermore, MultiLangMemeNet surpassed baseline multimodal techniques, achieving accuracy improvements of 6% in English (61% vs 55%), 2.68% in Bengali (66.02% vs 63.34%), 6% in Chinese (61% vs 55%), 4.2% in Hindi (73.28% vs 69.08%), and 2% in Tamil (47% vs 45%) compared to the next best multimodal approach. The study also explored early and late fusion strategies, revealing language-dependent variations in optimal fusion approaches. The findings indicate a significant advancement in multilingual meme sentiment analysis by demonstrating the efficacy of MultiLangMemeNet in capturing the complex interplay between visual and textual components in memes across various linguistic and cultural contexts.

# Summary. An optional shortened abstract.
summary: This study presents MultiLangMemeNet, a unified multimodal model for meme sentiment classification across five languages, achieving accuracy gains of 2.46%–13.74% over state-of-the-art unimodal and multimodal methods. The model demonstrates language-dependent fusion strategies and significant advancements in analyzing the visual-textual interplay in multilingual memes.


tags:
  - NLP

# Display this page in the Featured widget?
featured: true

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

url_pdf: 'https://www.researchgate.net/publication/385379332_MultiLangMemeNet_A_Unified_Multimodal_Approach_for_Cross-Lingual_Meme_Sentiment_Analysis'
url_code: ''
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
image:
  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/pLCdAaMFLTE)'
  focal_point: ''
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `internal-project` references `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects:
  - example

# Slides (optional).
#   Associate this publication with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides: "example"` references `content/slides/example/index.md`.
#   Otherwise, set `slides: ""`.
slides: example
---
